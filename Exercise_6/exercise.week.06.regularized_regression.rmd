---
title: "Week 06 - regularized regression/classification"
output:
  html_document: 
    theme: readable
editor_options: 
  chunk_output_type: console
---

# Learning goals

* Normal linear regression is meaningsless with k > n (more predictors than samples)
* Bias/Variance trade off. Overfitting is a problem with high dimensional data
* Use cross validation to estimate test error
* Try both lasso and ridge regression - and know the difference

# Data - phenotype prediction from blood metabolomics

We will work on public data where we try to predict two phenotypes of people - age and sex from different molecules in blood plasma.

The data is a mix of LCMS, GCMS and NMR data and it has all been combined here. The different names of the compounds are not super interesting for the purpose of this exercise.

## Data source

Metabolite patterns predicting sex and age in
participants of the Karlsruhe Metabolomics
and Nutrition (KarMeN) study

PLOS One

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0183228

# Book labs

Before proceeding - go to the book labs and go through 

6.5.2 Ridge Regression and the Lasso

then come back here.

(Maybe you could just write your code here - of you like)

And if you feel for it, you are welcome to go through the first part of the labs as well (subset selection etc).

```{r}

# Beginning of chapter 6 labs

library(ISLR2)

head(Hitters)

Hitters <- na.omit(Hitters)

```

# Karmen analysis

## Loading data

```{r}

library(tidyverse)

df <- read_rds(file = "karmen_data_for_week6.rds")

```

## Making a training/test split for all of us

To make it easier for all of us, we share some code so we save 20% of the data for validation (the same data for all of us)

```{r}

set.seed(0)

x <- model.matrix(age~., df)[, -1]
y <- df$age

df <- df %>%
  mutate(set = sample(x=c("Training set", "Test set"), size = n(), replace = TRUE, prob = c(0.80,0.20))) %>%
  select(set, everything())

train <- which(df$set=="Training set")
test <-  which(df$set=="Test set")

```

Now you have two vectors of row numbers called "train" and "test".

# Data inspection

## >Q1.1: How many predictors do we have in the data? (dimensions)

We have 441 variables.

--- 

## >Q1.2: How are the predictor values distributed? (scaling)

We will ignore any scaling requirements for now. But it's important that you can identify issues with data, e.g. are the data on different scales?

The quick and dirty check is to check the distribution of all the values combined and also log transformed.
```{r}
boxplot(x, outline = FALSE, main = "Boxplot of Predictor Variables", las = 2)


hist(as.vector(x), breaks = 50, main = "Distribution of All Predictors", xlab = "Values", col = "blue")

hist(log1p(as.vector(x)), breaks = 50, main = "Log-Transformed Predictor Distribution", xlab = "Log Values", col = "red")
```

I do see very different ranges so I would scale
--- 

## >Q1.3: How is our response (age) distributed? (transformation)

We should check if the dataset is balanced or looks weird.

```{r}
hist(y, breaks = 30, main = "Age Distribution", xlab = "Age", col = "skyblue", border = "black")

```


--- 

## >Q1.4: Make a PCA plot of the data, with age as color.

Do you see a pattern/tendency in the data? 


```{r}
# Standardize the predictors (x) before performing PCA
x_scaled <- scale(x)
# Perform PCA
pca_result <- prcomp(x_scaled, center = TRUE)
pca_scores <- pca_result$x
library(ggplot2)

# Combine PCA scores with the original data (age)
pca_data <- data.frame(PC1 = pca_scores[, 1], 
                       PC2 = pca_scores[, 2], 
                       Age = y)

# Create the PCA plot with Age as color
ggplot(pca_data, aes(x = PC1, y = PC2, color = Age)) +
  geom_point(alpha = 0.7) +
  labs(title = "PCA Plot of Data with Age as Color", x = "PC1", y = "PC2") +
  scale_color_gradient(low = "blue", high = "red") + # Color gradient based on age
  theme_minimal()



```

--- 

# Ridge regression

## >Q2.1: Perform cross validated ridge regression on the training data, calculate the training and test RMSE.


```{r}
library(glmnet)
# Using the training data for modeling
x_train <- x[train, ]  # predictors for training data
y_train <- y[train]    # response for training data

# Check if scaling is necessary, if already done we can use x_train as is

# Perform cross-validation for Ridge Regression
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)  # alpha = 0 for Ridge

# The best lambda (penalty parameter) based on cross-validation
best_lambda <- cv_ridge$lambda.min

# Fit the ridge model using the best lambda from cross-validation
ridge_model <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda)

# Make predictions on the training data
y_train_pred <- predict(ridge_model, s = best_lambda, newx = x_train)

# Calculate RMSE for training data
train_rmse <- sqrt(mean((y_train - y_train_pred)^2))
train_rmse

# Get the test data
x_test <- x[test, ]  # predictors for test data
y_test <- y[test]    # response for test data

# Make predictions on the test data
y_test_pred <- predict(ridge_model, s = best_lambda, newx = x_test)

# Calculate RMSE for test data
test_rmse <- sqrt(mean((y_test - y_test_pred)^2))
test_rmse

# Plot the cross-validation results for ridge regression
plot(cv_ridge)



```


There are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are: Mean Squared Error (MSE). Root Mean Squared Error (RMSE). Mean Absolute Error (MAE).


--- 

## >Q2.2: Find to top 10 molecules with the largest effect sizes

Hint: again, check the book labs to get coefficients from the model.


```{r}
# Extract coefficients for the fitted ridge regression model using the best lambda
coefficients <- coef(ridge_model, s = best_lambda)

# Convert the coefficients to a data frame for easier manipulation
coefficients_df <- data.frame(molecule = rownames(coefficients), coefficient = as.vector(coefficients))

# Remove the intercept (first row) since it is not a predictor
coefficients_df <- coefficients_df[-1, ]

# Sort the coefficients by absolute value in descending order
coefficients_df <- coefficients_df[order(abs(coefficients_df$coefficient), decreasing = TRUE), ]

# Get the top 10 molecules with the largest absolute effect sizes
top_10_molecules <- coefficients_df[1:10, ]

# Display the top 10 molecules
print(top_10_molecules)

```

--- 



# Lasso regression

## >Q3.1: Perform cross validated lasso regression on the training data, calculate the training and test RMSE.

```{r}
library(glmnet)

# Prepare the training and test data
x_train <- x[train, ]  # predictors for training data
y_train <- y[train]    # response for training data
x_test <- x[test, ]    # predictors for test data
y_test <- y[test]      # response for test data

# Perform cross-validation for Lasso Regression (alpha = 1 for Lasso)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10)  # alpha = 1 for Lasso

# The best lambda (penalty parameter) based on cross-validation
best_lambda_lasso <- cv_lasso$lambda.min

# Fit the lasso model using the best lambda from cross-validation
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda_lasso)

# Make predictions on the training data
y_train_pred_lasso <- predict(lasso_model, s = best_lambda_lasso, newx = x_train)

# Calculate RMSE for training data
train_rmse_lasso <- sqrt(mean((y_train - y_train_pred_lasso)^2))
train_rmse_lasso

# Make predictions on the test data
y_test_pred_lasso <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)

# Calculate RMSE for test data
test_rmse_lasso <- sqrt(mean((y_test - y_test_pred_lasso)^2))
test_rmse_lasso

# Plot the cross-validation results for lasso regression
plot(cv_lasso)

```


--- 

## >Q3.2: Find to top 10 molecules with the largest effect sizes

Hint: again, check the book labs to get coefficients from the model.

```{r}

```

--- 

# Comparison of lasso and ridge

## >Q4.1: Which method had lowest test RMSE?

Discuss the training error (RMSE) differences between the two. What happened?


```{r}
test_rmse
test_rmse_lasso
train_rmse
train_rmse_lasso
```

## >Q4.2: What does it tell you? 

So, think about if you expect all, some or few of the molecules to change with age. Is your expectation in line with what you observe?

## >Q4.3: What about non linearity and interactions ?

Do the chosen method allow for non-linear relations? What about interactions in the data?

(We will come back to this dataset later on).

## >Q4.3: What should we do? 

We do actually want to use data very similar to this find biomarkers we can use for estimating the age of blood spot from crime scenes. 

Which method would you prefer? Our goal is to find <25 target molecules, since we have super sensitive methods for measuring a few molecules with insane precision (targeted LCMS).

You can read a little about the project here (in Danish - you can probably get google to translate it): 

https://innovationsfonden.dk/da/i/historier/ny-retskemisk-metode-skal-afsloere

# Bonus

## >Q5: how robust are the error estimates for ridge and lasso if you have different train/test sizes?

So... basically I'm curious to see how much your estimates changes from run to run as well as with the size of the train/test set.

I the beginning of this exercise we set the training part of the data to be 80% and the test part to be 20%.

What do you think will happen if we use e.g. 50:50 ? (Then we have less training data but more test data.)


--- 

