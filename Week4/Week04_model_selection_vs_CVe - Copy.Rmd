---
title: "Week 04 Statistical Machine Learning in Bioinformatics: Model Selection vs Cross Validation"
author: "Thomas & Iker"
date: " Feb 2022, last update: `r Sys.Date()`"
output:
  html_document: 
    theme: readable
editor_options: 
  chunk_output_type: inline
---


---
# Goals for the R session 

* Practice model selection on a simulated example 

* Apply model selection to the ALS regression dataset

* Compare models selection relying on AIC with model choice using Cross validation 


# A first glimpse of model selection on a simulated data

>Q0 Simulate a dataset 

Simulate data (and save it as dataframe or tibble) consisting of 200 observations where you have a highly non linear relationship between predictors: 
$y=\beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \epsilon$, where $x$ $\epsilon$ are sampled in normal distribution with mean $\mu = 0$ and $\sigma = 1$ .

```{r}
###Coefficients 
beta_0 <- 3
beta_1 <- 2
beta_2 <- -3
beta_3 <- 0.3

```

Hint here is how you do it for a regression data that is assuming 
$y=\beta_0 + \beta_1 x + \epsilon$

So , feel free to adapt reuse :-) :

```{r}
set.seed(1)
X <- rnorm(200) 
epsilon <- rnorm(200)

# Non-linear relationship
Y <- beta_0 + beta_1 * X + beta_2 * X^2 + beta_3 * X^3 + epsilon

# Create dataframe
quick_df <- data.frame(X, Y)

# View first few rows
head(quick_df)
```



In order to generatre data where you can "see" by eye a pattern, lets fix the variance of $x$ to 5 and the variance of $\epsilon$ to 1. 

Hint: you can simulate these deviates ( $x$ $\epsilon$ using the rnorm() function)

>Q1 Fit sequentially models with increasingly many polynomial terms

Try first to fit a linear 
$y=\beta_0 + \beta_1 x + \epsilon$,
then
$y=\beta_0 + \beta_1 x + \beta_2 x^2  + \epsilon$,
and so on ... up to 
$y=\beta_0 + \beta_1 x + \beta_2 x^2 + ...+ \beta_{10} x^{10} + \epsilon$,

What is the best model obtained according to BIC, and adjusted R2? 

Hints: 

A good trick is to use the function poly() to write quickly polynomial terms
For instance:

AQUÍ LO Q HACEMOS ES: 

- DECIDIR UNA FUNCIÓN (MODELO REAL):Y
- PONERLE RANDOM IMPUTS:X
- PONERLE RANDOME ERRORES: EPSILON

AHORA EVALUAMOS EL FORWARD Y EL BACKWARD MODEL SELECTION PARA CADA UNO. 

```{r cars}
quick_df <- data.frame(x=rnorm(200), y=rnorm(200))
my_formula <- "y ~ poly(x, 4)"
model4 <- lm(data = quick_df, my_formula) 
names(model4)
(myAIC <- extractAIC(model4, k=2)) # check the help 

```





Note You can also use the library leaps and regsubsets() and summaries to get these indices

See the syntax is the R lab 1 (book chapter 6.5).

>Q2 Vizualize the model selection using  plots 

Make a series of plot that are inspired by the ones presented in the book: 

* The number of parameters in the model as x axis and 

* cp, BIC or adj R^2 of the model as y axis 

```{r}
library(tidyverse)  # For data manipulation and visualization
library(patchwork)



# Store results
models <- list()
bic_values <- c()
adj_r2_values <- c()

# Fit polynomial models from degree 1 to 10
for (i in 1:10) {
  formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
  model <- lm(formula, data = quick_df)
  
  # Save the model
  models[[i]] <- model
  
  # Calculate BIC
  bic_values[i] <- extractAIC(model, k = log(nrow(quick_df)))[2]
  
  # Get adjusted R^2
  adj_r2_values[i] <- summary(model)$adj.r.squared
}

# Create a data frame with the results
results <- data.frame(
  Degree = 1:10,
  BIC = bic_values,
  Adjusted_R2 = adj_r2_values
)

# Display the results
print(results)

# Plot BIC vs. Number of Parameters
a<-ggplot(results, aes(x = Degree, y = BIC)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  labs(title = "BIC vs. Number of Parameters",
       x = "Number of Parameters",
       y = "BIC") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Plot Adjusted R^2 vs. Number of Parameters
b<-ggplot(results, aes(x = Degree, y = Adjusted_R2)) +
  geom_line(color = "red") +
  geom_point(color = "red") +
  labs(title = "Adjusted R^2 vs. Number of Parameters",
       x = "Number of Parameters",
       y = "Adjusted R^2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Combine the plots side by side
combined_plot <- a+b

# Display the combined plot
print(combined_plot)


```

AHORA PROBAMOS CON BACKWARD SELECTION A VER SI OBTENEMOS EL MODELO REAL O NO, COMO VEMOS OBTENEMOS EN EL FORWARD OBTENEMOS EL NUMERO REAL DE PREDICTORES, COMO ÓPTIMO, 3 PREDICTORES.



>Q3 Repeat (Q1 and Q2), using using backwards stepwise selection. 

How does your answer vary with the different strategies (subset) ? 


AHORA HAY Q PROBAR ESTE MÉTODO A VER SI TAMBIÉN NOS DA 3 PREDICTORES.

```{r}

library(MASS)

# Fit the full model (including up to X^10)
full_model <- lm(Y ~ poly(X, 10), data = quick_df)
summary(full_model)

# Perform Backward Stepwise Selection using BIC as the criterion
best_model <- stepAIC(full_model, direction = "backward", k = log(nrow(quick_df)))

# Extract BIC and Adjusted R^2 for the best model
best_bic <- extractAIC(best_model, k = log(nrow(quick_df)))[2]
best_adj_r2 <- summary(best_model)$adj.r.squared

# Print the results
cat("Best Model (Backward Selection):\n")
print(summary(best_model))

cat("\nBIC:", best_bic)
cat("\nAdjusted R^2:", best_adj_r2)

# Extract number of parameters
num_params <- length(coef(best_model))

# Create a results dataframe for visualization
results_bss <- data.frame(
  Parameters = num_params,
  BIC = best_bic,
  Adjusted_R2 = best_adj_r2
)

# Load patchwork for combining plots
library(patchwork)

# Plot BIC vs Number of Parameters
plot_bic_bss <- ggplot(results_bss, aes(x = Parameters, y = BIC)) +
  geom_point(color = "blue", size = 3) +
  labs(title = "BIC vs. Number of Parameters (BSS)",
       x = "Number of Parameters",
       y = "BIC") +
  theme_minimal()

# Plot Adjusted R^2 vs Number of Parameters
plot_adj_r2_bss <- ggplot(results_bss, aes(x = Parameters, y = Adjusted_R2)) +
  geom_point(color = "red", size = 3) +
  labs(title = "Adjusted R^2 vs. Number of Parameters (BSS)",
       x = "Number of Parameters",
       y = "Adjusted R^2") +
  theme_minimal()

# Combine the plots
plot_bic_bss + plot_adj_r2_bss



```



```{r}

# Load necessary packages
library(MASS)  # For stepAIC()
library(ggplot2)  # For visualization

# Fit models with increasing polynomial terms
models_3 <- list()
max_degree <- 5  # Adjust as needed

for (d in 1:max_degree) {
  formulas <- as.formula(paste("Y ~ poly(X,", d, ")", sep = ""))
  models_3[[d]] <- lm(formulas, data = quick_df)
}

# Here we already have all of the different possible models

# Start with the most complex model
full_model <- models_3[[max_degree]]  

# Perform stepwise selection and track models along the way
stepwise_path <- list()
stepwise_BIC <- c()

step_model <- full_model  # Start with full model
step_num <- 1

repeat {
  # Store current model
  stepwise_path[[step_num]] <- step_model
  stepwise_BIC[step_num] <- BIC(step_model)
  
  # Try the next step
  next_model <- update(step_model, . ~ . - (tail(names(coef(step_model)), 1)))  # Drop last term
  next_BIC <- BIC(next_model)
  
  # If BIC increases, stop
  if (next_BIC > stepwise_BIC[step_num]) break
  
  # Move to next step
  step_model <- next_model
  step_num <- step_num + 1
}

# Create a dataframe for plotting
plot_data <- data.frame(
  Num_Parameters = sapply(stepwise_path, function(m) length(coef(m))),
  BIC = stepwise_BIC
)

# Plot
ggplot(plot_data, aes(x = Num_Parameters, y = BIC)) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  labs(title = "Model Selection using BIC",
       x = "Number of Parameters",
       y = "BIC")
```


The 11 coefficients represent the intercept plus 10 polynomial terms generated by using poly(X, 10). Even if only the first 4 coefficients are statistically significant (i.e., have p-values below a conventional threshold), the model still includes all 11 parameters because:

Backward Selection with BIC:
The backward elimination procedure using stepAIC (with BIC as the criterion) did not remove any terms. This means that even though some coefficients have high p-values, removing them did not improve the overall model (as measured by BIC). In other words, the best model according to the BIC criterion still kept all 10 polynomial terms.
Orthogonal Polynomials:
The function poly(X, 10) generates 10 orthogonal polynomial basis functions. Their individual p-values can sometimes be misleading because these basis functions are constructed to be uncorrelated, and the significance of one term does not directly indicate that higher-order terms are unnecessary for the model's overall fit.
Statistical vs. Model Selection Criteria:
Model selection (here based on BIC) balances model fit with complexity. Although some coefficients are not statistically significant on their own, the combined model might still be the best compromise between bias and variance according to BIC.
Thus, when you run length(coef(best_model)) and get 11, it tells you that your final model (as selected by backward elimination with BIC) includes the intercept plus all 10 polynomial terms—even if only 4 of them show statistical significance individually.
>Q4 exploring the selection criteria via simulations

* Generate 100 different datasets as in Q0

* In how many datasets did you select the right model using the BIC criteria? 

```{r}

### Q4: Simulation for Model Selection Accuracy
correct_selections <- 0
num_simulations <- 100

for (sim in 1:num_simulations) {
  X_sim <- rnorm(n, mean = 0, sd = sqrt(5))
  epsilon_sim <- rnorm(n, mean = 0, sd = 1)
  Y_sim <- beta_0 + beta_1 * X_sim + beta_2 * X_sim^2 + beta_3 * X_sim^3 + epsilon_sim
  df_sim <- data.frame(X = X_sim, Y = Y_sim)
  
  bic_values_sim <- c()
  for (i in 1:10) {
    model_sim <- lm(Y ~ poly(X, i), data = df_sim)
    bic_values_sim[i] <- extractAIC(model_sim, k = log(nrow(df_sim)))[2]
  }
  
  best_model_index <- which.min(bic_values_sim)
  if (best_model_index == 3) {
    correct_selections <- correct_selections + 1
  }
}

accuracy <- correct_selections / num_simulations
cat("Accuracy of Model Selection using BIC:", accuracy * 100, "%\n")

```



# ALS data again 

>Q5 Use AIC and forward selection to select a regression model on the ALS dataset.

```{r}
library(MASS)

# Check for missing values
als_data <- na.omit(`ALS_progression_rate.1822x370 - Copy`)

# Ensure all variables have the same number of observations
als_data <- als_data[complete.cases(als_data), ]

# Ensure 'dFRS' is numeric
als_data$dFRS <- as.numeric(as.character(als_data$dFRS))

# Run the model again
full_model <- lm(Y ~ ., data = als_data)

# Perform Forward Selection
forward_model <- stepAIC(lm(Y ~ 1, data = als_data),  
                         scope = formula(full_model),  
                         direction = "forward")

# Print the final selected model
summary(forward_model)


```


Compare the AIC of the model you selected with the AIC of the model you used for your first round of submissions
Use the cross validation procedure you used to obtain the error rate on the model you just selected . 
Do you get a different error rate for this model ( relative to your first submission) ? 

>Suppl. Q How are cross validation errors and AIC criteria related ? (If time allows)

When we build a statistical model for future predictions, we want to fit the data ... without over-fitting so we can make reliable prediction on future data. To do so, we have to "strategies" to build a sensible model: 

* Use cross validation by splitting our data in a training and validation set. This is the golden standard; but can be compuationally costly 
* Use model selection criteria ( such as AIC, BIC etc) to select models directly 

Use 10-fold cross validation to obtain the CV error of each model. 
How does the ranking of models based on Cvs compare with the ranking of models based on AIC ?


Note, as an outlook to coming weeks, you will discuss another strategy: model regularization "as you fit" (see lasso and ridge regressions in ch 6.2)



